---
title: "FTARM"
authors: "Sharon, Jason, Jayakrishna and Trilok"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FTARM}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# FTARM Algorithm

# Introduction
**Does your vignette start out by explaining why someone should want to use your package**

FTARM (Fast Top-K Association Rule Mining) is a proposed algorithm that does association rule mining. Association rule mining is a branch of data mining that deals with the identification of useful patterns within a given dataset. Compared with other existing algorithms, FTARM is said to be more efficient as it automates most of the procedures, thus, producing patterns with improved accuracy. Our package intends to further improve the accuracy of FTARM by fully automating the algorithm. Also, using the existing apriori algorithm, we intend to help the user identify the optimal model that produces the most interpretable set of rules. 

# Significance of the Package
**Does your vignette make the case for why this project is important? Does it highlight what you've done differently than other resources that are currently available?**


There is only one algorithm (apriori) in R programming language that does association rule mining. The apriori algorithm is found inside the Arules package. With the Apriori algorithm, the user is required to provide the key parameters (minimum support and minimum confidence) that determine the number of rules / patterns that are generated by the algorithm. This is a major drawback with apriori algorithm, as well as with the other existing algorithms, because it is quite tedious for the users to keep on adjusting the parameters till they find the needed to produce the optimal model that contains the desired set of rules. The proposed FTARM algorithm eliminates the need for the user to provide the minimum support value, however, the user is still required to provide the minimum confidence value. Therefore, our proposed package intends to completely automate the FTARM algorithm by eliminating the need for the user to provide any of the key parameters.   

# Demonstration of the Package
**Does your vignette explain each function in your package and show how to use it with code examples? You do not need to be completely exhaustive (your documentation should be self-explanatory) but you do need to at least demonstrate usage for the general case.**

Our success was dependent on the implementation and functioning of the FTARM algorithm. However, given that only the pseudo code for the FTARM algorithm existed, we had to implement the algorithm from the ground up. FTARM was dependent on many functions for it to work. We tried to implement all the dependent functions, however, we only managed to implement a couple of them that will be shown below.

## Demonstration of association rule mining

We used the existing Apriori algorithm to demonstrate association rule mining and to show how the FTARM algorithm is expected to work after it's successful implementation:

```{r, message = FALSE, warning = FALSE}
# Load packages

library(arules)
library(knitr)
library(arulesViz)
library(dplyr)
```

```{r}
# Load data
# The arules package comes with the 'Groceries' dataset.
data(Groceries)
```

```{r, results = 'hide', message=FALSE}
# A demonstration of ssociation rule mining with the use of apriori algorithm.
# We tried different values of minimum support and minimum confidence.
# We found the minimum support value of 0.002 and minimum confidence value of 0.05 to work best.
model = apriori(data = Groceries, 
                parameter = list(support=0.002, confidence=0.05))
```

A lot of rules, 10124 rules, are generated by the apriori algorithm. Interpreting and visualizing all the rules is quite challenging. As seen in the scatterplot below, there is a lot of overlap when visualizing the rules. We are only able to visualize a couple rules as shown in the graph visualization below.  

```{r}
plot(model)
```



```{r, warning = FALSE, message = FALSE, fig.width = 6, fig.height = 4}
# Visualization of some of the rules
#Visualize the top 10 rules with the highest lift and plot them
highLiftRules<-tail(sort(model,by="lift"),15)
plot(highLiftRules,method="graph",control=list(type="items"))
```

```{r}
#plot(model)
#plot(model, method = "grouped", control = list(k = 5))
#plot(model, method="graph", control=list(type="items"))
#plot(model, method="paracoord",  control=list(alpha=.5, reorder=TRUE))
#plot(model,measure=c("support","lift"),shading="confidence",interactive=T)
```


**Do you use real datasets that are easy to load (or included in your package's `/data` directory), are they nicely analyzed, and are the examples intuitive? Did you get results that make sense? Are they interesting?**
When implementing the FTARM algorithm, We used the [movie_lens](2.	https://grouplens.org/datasets/movielens/) dataset, which is publicly available. Only a subset of the data, with 400 observations was used. The sample data contains the list of movies that the users gave a rating of 5. The sample data was stored in a csv file named `movie_ratings.csv`, which is available on our [github](https://github.com/skirwa/AutoFTRAM) repository.

```{r}
data_nmf <- read.csv("C:/Users/skirw/Desktop/Academics/AutoFTRAM/User_Ratings.csv")
```

```{r}
set.seed(100)  # Setting  seed ensures reproducability.
data_nmf <- RunNMF(data_nmf)
```


## Demonstration of FTARM and the implemented Functions

Below is a demonstration of FTARM's performance compared to the performance of other algorithms. As seen in figure 1 below, for different datasets (Pumsb and Connect), FTARM has better runtime and lower memory consumption compared to TopKrules and ETARM algorithm (Liu et al., 2021). Its better performance is owed to the fact that it produces much less rules that are more interpretable (Liu et al., 2021).

![Figure 1: FTARM's Performance](C:/Users/skirw/Pictures/Screenshots/FTARM.png)

Below is a demonstration of the FTARM's dependent functions that we were able to implement:

1) The tids functions - tids is the transaction identifier of a set of items. Tids determines how frequently an item appears in the dataset. For the tids function, we plan on writing one function that takes in either one item or a set of items. 

Loading the sample data.
```{r}
library(singlet)
library(Seurat)
library(dplyr)
library(ggplot2)
```

```{r}
data_sample <- read.csv("C:/Users/skirw/Desktop/Academics/AutoFTRAM/movie_ratings.csv")
```


```{r}
# Tids is the transaction identifier of a set of items.
# tids(x) - how many transactions (columns) have item x?
tids <- function(searchFor, DB) {
  # movie_ratings implementation: count how many columns x appears in
  DB <- as.list(DB)
  result <- lapply(DB, function(x) {
    sum(x == searchFor)
  })
  
  total <- 0
  for(i in result) {
    if(i[1] == 1) {
      total <- total + 1
    }
  }
  return(total)
}

# The outcome tells us that 9 users have watched the movie 'Toy Story' (with movie_id = 1). 
tids(1, data_sample)

```

```{r}
tidsForTwo <- function(searchFor1, searchFor2, DB) {
  # calculate the number of columns or transactions that contain x and y
  DB <- as.list(DB)
  result <- lapply(DB, function(x) sum(x == searchFor1) != 0 && sum(x == searchFor2) != 0)
  
  total <- 0
  for(i in result) {
    if(i[1]) {
      total <- total + 1
    }
  }
  return(total)
}

# The outcome tells us that 3 users have watched both 'Toy Story' (id = 1), and 'Grumpier' (id=3).
tidsForTwo(1, 3, data_sample)
```

2) The sup() function - Used to calculate the support of an item.
```{r}
# Calculate the support value for an item.
sup <- function(DB, i) {
  # Support is the no of transactions (I) in DB with item i / total number of transactions in DB.   
  support <- tids(i, DB) / length(DB)
  return(support)
}
# The support value helps to identify the set of rules.
# The support of the movie 'Shawshank Redemption' is 0.35
sup(data_sample, 318)
```

3) The conf() function - Used to calculate the confidence level of an item
```{r}
# Calculate the confidence value for an item
conf <- function(DB, x, y) {
    # Confidence is the no of transactions with items x and y / no of transactions with item x.
    confidence = tidsForTwo(x, y, DB) / tids(x, DB)
    return (confidence)
}

# The confidence that a user will watch both the movies 'Toy Story' (id = 1) and 'Grumpier' (id=3) is 0.3333.
conf(data_sample, 1, 3)

```

4) The minConf() function - Used to calculate the minimum confidence level.
```{r}
# A list of items should be parsed into minConf and minSup
# However, we were unable to write a tids function that takes in a list.
# Therefore, for minConf and minSup, we used the tidsForTwo function instead.
# The minConf function is supposed to calculate the minimum confidence value.
minConf <- function(DB, searchFor1, searchFor2) {
  maxTid = -1
  x <- list()
  x <- append(x, searchFor1, searchFor2)
  for (i in x) {
    if (tids(i, DB) > maxTid) {
      maxTid = tids(i, DB)
    }
  }
  return(tidsForTwo(searchFor1, searchFor2, DB)/maxTid)
}

minConf(data_sample, 260, 318)
```

5) The minSup() function - Used to comoute the minimum support value.
```{r}
# The minSup function calculates the minimum support value.
minSup <- function(x, data) {
   return(tids(x, data)/length(data))
}

minSup(1, data_sample)
```

6) The SortItems() function - Used to sort items in descending order based on  their support value.
```{r}
# Function for sorting the items in the dataset based on their support value
SortItems <- function(DB) {
  # Define an empty vector
  itemsinData <- c()
  # Store the ids of the items in the database in the empty vector
  for (i in DB){
    itemsinData <- c(itemsinData, i)
  }
  # Convert the list of items to a dataframe
  new_df <- data.frame(sapply(itemsinData,c))
  # Rename the column with ids
  colnames(new_df)[1] ="id"
  # This function returns the support value for each item.
  func <- function(new_df){
    return(sapply(new_df$id, function(id) sum(new_df$id == id)/nrow(new_df)))
  }
  # Create a new column that contains the support values of the corresponding items.
  new_df$support <- func(new_df)
  # Sort the items in descending order based on their support values
  swapped <- new_df %>% arrange(desc(support))
  swapped
}

tail(SortItems(data_sample), 5)
```



We acknowledge the fact that we took on a challenging task of trying to implement a novel algorithm within a month. But it's been a learning curve and 

# visualizations
**Does your package vignette make use of pretty visualizations to make your points? It is possible that your project does not rely much on visualizations, and so not as much will expected, although you should still try to use other packages to generate some figures (e.g. benchmarking runtime, comparison of results with other packages, etc.). However, if your package has function outputs that either could use visualizations or are themselves visualization functions, make sure that your plots look clean and are self-explanatory.**


Visualizing Useful Patterns (Addresses Aim 2)

References
1. Liu, X., Niu, X., & Fournier-Viger, P. (2021). Fast top-k association rule mining using rule generation property pruning. Applied Intelligence, 51(4), 2077-2093.
